<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Compact Action Token, Faster Auto Regression">
  <meta name="keywords" content="Action Representation, Vision-Language-Action Models, Auto Regression">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OmniSAT</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <style>
    .container.is-custom-width {
      max-width: 100%; /* Override Bulma's default max-width */
      padding: 0 2rem;
    }
    figure img {
      max-width: 100%; /* Ensure the image scales correctly */
    }
  </style>
  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <!-- <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          </a>
        </div>
      </div>
    </div>

  </div> -->
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">OmniSAT: Compact Action Token, Faster Auto Regression</h1>
          <div class="is-size-5 publication-authors">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Anonymous Authors</span>
            </div>

          <!-- <div class="column has-text-centered">
            <div class="publication-links"> -->
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
            <!-- </div>

          </div> -->
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-custom-width">
    <!-- Teaser PDF -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="box m-5">
          <div class="content has-text-centered">
            <h3 class="title is-4">Comparison between Existing Approaches and OmniSAT.</h3>
            <figure>
              <img src="./static/teaser-1.png" alt="Teaser" style="width:100%; max-width:800px;">
            </figure>
            <p class="has-text-left" style="margin-top: 1rem;">
              (a) Diffusion-based policies require iterative denoising, limiting training efficiency and scalability.
              (b) AR policies train efficiently and support flexible sequence construction, but sacrifice fine-grained accuracy in continuous control.
              (c) OmniSAT amplifies AR efficiency through feasible high-rate compression while providing a unified token space that enables integration of heterogeneous datasets.
            </p>
          </div>
        </div>
      </div>
    </div>


    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Existing Vision-Language-Action (VLA) models can be broadly categorized into diffusion-based and auto-regressive (AR) approaches: diffusion models capture continuous action distributions but rely on computationally heavy iterative denoising. In contrast, AR models enable efficient optimization and flexible sequence construction, making them better suited for large-scale pretraining. To further improve AR efficiency, particularly when action chunks induce extended and high-dimensional sequences, prior work applies entropy-guided and token-frequency techniques to shorten the sequence length. 
            However, such compression struggled with <i>poor reconstruction or inefficient compression</i>.
            Motivated by this, we introduce an <b>O</b>mni <b>S</b>wift <b>A</b>ction <b>T</b>okenizer, which learns a compact, transferable action representation.
            Specifically, we first normalize value ranges and temporal horizons to obtain a <b>consistent representation</b> with B-Spline encoding.
            Then, we apply multi-stage residual quantization to the position, rotation, and gripper subspaces, producing <b>compressed discrete tokens with coarse-to-fine granularity</b> for each part.
            After pre-training on the large-scale dataset Droid, the resulting discrete tokenization shortens the training sequence by <b>6.8</b>×, and lowers the target entropy.
            To further explore the potential of <b><i>OmniSAT</i></b>, we develop a cross-embodiment learning strategy that builds on the unified action-pattern space and jointly leverages robot and human demonstrations. It enables scalable auxiliary supervision from heterogeneous egocentric videos.
            Across diverse real-robot and simulation experiments, OmniSAT encompasses higher compression while preserving reconstruction quality, enabling faster AR training convergence and model performance.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->


    <!-- SAT Diagram -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="box m-5">
          <div class="content has-text-centered">
            <h3 class="title is-4">OmniSAT Tokenization Pipeline</h3>
            <figure>
              <img src="./static/sat-1.png" alt="SAT Diagram" style="width:100%; max-width:800px;">
            </figure>
            <p class="has-text-left" style="margin-top: 1rem;">
              <strong>Consistency Encoding</strong> converts variable-length trajectories into temporally aligned, fixed-length control-point representations via B-spline fitting.
              <strong>Quantization Compression</strong> splits control-point features into part groups (position, rotation, gripper) and applies residual vector quantization to obtain layerwise codebook indices. The selected indices are then flattened into final compact action-pattern tokens.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ SAT Diagram -->

    <!-- <div class="content has-text-centered">
      <h2 class="title is-3"><strong>Retry Phenomenon Example</strong></h2>
    </div>
    <div class="container is-max-widescreen">
      <div class="box m-5">
          
        <div class="content has-text-centered">
          <div class="columns is-centered">
            <div class="column is-half">
              <video controls autoplay muted loop playsinline style="width:100%; max-width:400px;">
                <source src="./static/videos/success_example.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
              <p><strong>With H2R-BM</strong></p>
            </div>
            <div class="column is-half">
              <video controls autoplay muted loop playsinline style="width:100%; max-width:400px;">
                <source src="./static/videos/fail_example.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
              <p><strong>Without H2R-BM</strong></p>
            </div>
          </div>
        </div>
    
        <div class="content has-text-centered">
          <div class="columns is-centered">
            <div class="column is-half">
              <video controls autoplay muted loop playsinline style="width:100%; max-width:400px;">
                <source src="./static/videos/groceries_success.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
              <p><strong>With H2R-BM</strong></p>
            </div>
            <div class="column is-half">
              <video controls autoplay muted loop playsinline style="width:100%; max-width:400px;">
                <source src="./static/videos/groceries_failure.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
              <p><strong>Without H2R-BM</strong></p>
            </div>
          </div>
        </div>
        
      </div>
    </div> -->

    
    <!-- Method Diagram -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="box m-5">
          <div class="content has-text-centered">
            <h3 class="title is-4">OmniSAT for Cross-Embodiment Manipulation Learning.</h3>
            <figure>
              <img src="./static/method-1.png" alt="Method Diagram" style="width:100%; max-width:800px;">
            </figure>
            <p class="has-text-left" style="margin-top: 1rem;">
              The training pipeline has two phases:
              (i) <strong>Tokenizer Pretraining</strong>: OmniSAT is pretrained on heterogeneous human–robot datasets to learn a unified and compressed (× 6.8) action token space;
              (ii) <strong>Cross-Embodiment Fine-Tuning</strong>: we construct mixed visual-action auto-regressive sequences over OmniSAT token space, enabling efficient and scalable fine-tuning through shorter sequences and lower target entropy.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!--/ Method Diagram -->
    
  <!-- Example Training Data -->
    <!-- <div class="columns is-centered">
      <div class="container">
        <div class="content has-text-centered">
          <h2 class="title is-3"><strong>Example Training Data</strong></h2>
          <div class="box m-5">
            <div class="columns is-centered">
              
              <!-- Human Video -->
              <!-- <div class="column is-half">
                <video controls autoplay muted loop playsinline style="width:100%; max-width:400px;">
                  <source src="./static/videos/human_example.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
                <p><strong>Human Video</strong></p>
              </div>
              <!-- Robot Video -->
              <!-- <div class="column is-half">
                <video controls autoplay muted loop playsinline style="width:100%; max-width:400px;">
                  <source src="./static/videos/robot_example.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
                <p><strong>Robot Video</strong></p>
              </div>
              
            </div>
          </div>
        </div>
      </div>
    </div> -->
    <!--/ Example Training Data -->
    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>
<!-- 
<section class="section">
  <div class="container is-max-desktop">
 -->
    <!-- <div class="columns is-centered"> -->

      <!-- Visual Effects. -->
<!--       <div class="column">
        <div class="content">
          <h2 class="title is-3">Human Example</h2>
          <p>
            Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
            would be impossible without nerfies since it would require going through a wall.
          </p>
          <video id="human example" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/human_example.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div> -->
      <!--/ Visual Effects. -->

      <!-- Matting. -->
<!--       <div class="column">
        <h2 class="title is-3">Robot Example</h2>
        <div class="columns is-centered">
          <div class="column content">
             <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p> 
            <video id="robot_example" controls playsinline height="100%">
              <source src="./static/videos/robot_example.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div> 
    </div> -->
    <!--/ Matting. -->

    <!-- Animation. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width"> -->
        <!-- <h2 class="title is-3">Animation</h2> -->

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3> -->
        <!-- <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div> -->
        <!-- <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div> -->
        <!-- <br/> -->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ Re-rendering. -->

      <!-- </div>
    </div> -->
    <!--/ Animation. -->

  <!-- </div> -->
<!-- </section> -->



<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
<!--     <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Source code is from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
