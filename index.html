<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Compact Action Token, Faster Auto Regression">
  <meta name="keywords" content="Action Representation, Vision-Language-Action Models, Auto Regression">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>OmniSAT</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <style>
    .container.is-custom-width {
      max-width: 100%; /* Override Bulma's default max-width */
      padding: 0 2rem;
    }
    figure img {
      max-width: 100%; /* Ensure the image scales correctly */
    }
  </style>
  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">OmniSAT: Compact Action Token, Faster Auto Regression</h1>
          <div class="is-size-5 publication-authors">
            <div class="is-size-5 publication-authors">
              <span class="author-block">Anonymous Authors</span>
            </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-custom-width">
    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="publication-video">
          <video controls autoplay muted loop playsinline style="width:100%; max-width:800px;">
            <source src="./static/ICLR.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
    </div>
    <!--/ Paper video. -->

    <!-- Teaser Image -->
    <div class="columns is-centered">
      <div class="container">
        <div class="content has-text-centered">
          <h2 class="title is-3"><strong>Comparison between Existing Approaches and OmniSAT</strong></h2>

          <div class="box m-5">
            <div class="content has-text-centered">
              <figure>
                  <img src="./static/teaser-1.png" alt="Teaser" style="width:90%;">
                <figcaption>
                  (a) Diffusion-based policies require iterative denoising, limiting training efficiency and scalability.
                  (b) AR policies train efficiently and support flexible sequence construction, but sacrifice fine-grained accuracy in continuous control.
                  (c) OmniSAT amplifies AR efficiency through feasible high-rate compression while providing a unified token space that enables integration of heterogeneous datasets.
                </figcaption>
              </figure>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Abstract and Real-world Experiments -->
    <div class="container is-custom-width">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Existing Vision-Language-Action (VLA) models can be broadly categorized into diffusion-based and auto-regressive (AR) approaches: diffusion models capture continuous action distributions but rely on computationally heavy iterative denoising. In contrast, AR models enable efficient optimization and flexible sequence construction, making them better suited for large-scale pretraining. To further improve AR efficiency, particularly when action chunks induce extended and high-dimensional sequences, prior work applies entropy-guided and token-frequency techniques to shorten the sequence length. 
              However, such compression struggled with <i>poor reconstruction or inefficient compression</i>.
              Motivated by this, we introduce an <b>O</b>mni <b>S</b>wift <b>A</b>ction <b>T</b>okenizer, which learns a compact, transferable action representation.
              Specifically, we first normalize value ranges and temporal horizons to obtain a <b>consistent representation</b> with B-Spline encoding.
              Then, we apply multi-stage residual quantization to the position, rotation, and gripper subspaces, producing <b>compressed discrete tokens with coarse-to-fine granularity</b> for each part.
              After pre-training on the large-scale dataset Droid, the resulting discrete tokenization shortens the training sequence by <b>6.8</b>Ã—, and lowers the target entropy.
              To further explore the potential of <b><i>OmniSAT</i></b>, we develop a cross-embodiment learning strategy that builds on the unified action-pattern space and jointly leverages robot and human demonstrations. It enables scalable auxiliary supervision from heterogeneous egocentric videos.
              Across diverse real-robot and simulation experiments, OmniSAT encompasses higher compression while preserving reconstruction quality, enabling faster AR training convergence and model performance.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Real-world Experiments -->
      <div class="content has-text-centered">
        <h2 class="title is-3"><strong>Real-world Experiments</strong></h2>
      </div>
      <div class="container is-max-widescreen">
        <div class="box m-5">
          <div class="content has-text-centered">
            <div class="columns is-centered">
              <div class="column is-one-third">
                <video controls autoplay muted loop playsinline style="width:100%; max-width:300px;">
                  <source src="./static/PlaceObj.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
                <p><strong>PlaceObj</strong></p>
              </div>
              <div class="column is-one-third">
                <video controls autoplay muted loop playsinline style="width:100%; max-width:300px;">
                  <source src="./static/TubeRack.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
                <p><strong>TubeRack</strong></p>
              </div>
              <div class="column is-one-third">
                <video controls autoplay muted loop playsinline style="width:100%; max-width:300px;">
                  <source src="./static/ZipSeal.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
                <p><strong>ZipSeal</strong></p>
              </div>
            </div>
          </div>
        </div>
      </div>
      <!--/ Real-world Experiments -->
    </div>

    <!-- SAT Diagram -->
    <div class="columns is-centered">
      <div class="container">
        <div class="content has-text-centered">
          <h2 class="title is-3"><strong>OmniSAT Tokenization Pipeline</strong></h2>

          <div class="box m-5">
            <div class="content has-text-centered">
              <figure>
                  <img src="./static/sat-1.png" alt="SAT Diagram" style="width:90%;">
                <figcaption>
                  <strong>Consistency Encoding</strong> converts variable-length trajectories into temporally aligned, fixed-length control-point representations via B-spline fitting.
                  <strong>Quantization Compression</strong> splits control-point features into part groups (position, rotation, gripper) and applies residual vector quantization to obtain layerwise codebook indices. The selected indices are then flattened into final compact action-pattern tokens.
                </figcaption>
              </figure>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!--/ SAT Diagram -->


    
    <!-- Method Diagram -->
    <div class="columns is-centered">
      <div class="container">
        <div class="content has-text-centered">
          <h2 class="title is-3"><strong>OmniSAT for Cross-Embodiment Manipulation Learning</strong></h2>

          <div class="box m-5">
            <div class="content has-text-centered">
              <figure>
                  <img src="./static/method-1.png" alt="Method Diagram" style="width:90%;">
                <figcaption>
                  The training pipeline has two phases:
                  (i) <strong>Tokenizer Pretraining</strong>: OmniSAT is pretrained on heterogeneous humanâ€“robot datasets to learn a unified and compressed (Ã— 6.8) action token space;
                  (ii) <strong>Cross-Embodiment Fine-Tuning</strong>: we construct mixed visual-action auto-regressive sequences over OmniSAT token space, enabling efficient and scalable fine-tuning through shorter sequences and lower target entropy.
                </figcaption>
              </figure>
            </div>
          </div>
        </div>
      </div>
    </div>
    <!--/ Method Diagram -->
    
  </div>
</section>






<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Source code is from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
